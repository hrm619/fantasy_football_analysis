{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 data preparation Notebook\n",
    "\n",
    "This notebook implements the analysis for the 00 data preparation stage of the Fantasy Football Analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preparation for Fantasy Football Analysis\n",
    "\n",
    "This script handles the initial data loading, cleaning, and preprocessing steps for the Fantasy Football Analysis project.\n",
    "It includes:\n",
    "1. Loading raw data files\n",
    "2. Standardizing team and player names\n",
    "3. Cleaning and preprocessing data\n",
    "4. Creating a master player dataset\n",
    "5. Calculating fantasy points in half-PPR format\n",
    "6. Saving processed datasets for further analysis\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to path to import project modules\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "# Import project modules\n",
    "from src.data.data_loader import load_config, load_csv_data, load_all_data\n",
    "from src.data.data_processor import (\n",
    "    standardize_team_names, \n",
    "    filter_season_data, \n",
    "    create_master_player_dataset,\n",
    "    calculate_half_ppr_points,\n",
    "    save_processed_data\n",
    ")\n",
    "from src.utils.validation import validate_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement 00 data preparation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_configuration():\n",
    "    \"\"\"Load configuration and extract important paths and settings.\"\"\"\n",
    "    # Load configuration\n",
    "    config = load_config()\n",
    "    logger.info(\"Configuration loaded\")\n",
    "    \n",
    "    # Print configuration details\n",
    "    for section, settings in config.items():\n",
    "        logger.info(f\"\\n{section.upper()}:\")\n",
    "        if isinstance(settings, dict):\n",
    "            for key, value in settings.items():\n",
    "                logger.info(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            logger.info(f\"  {settings}\")\n",
    "    \n",
    "    # Extract important paths\n",
    "    raw_data_path = config['data_paths']['raw_data']\n",
    "    processed_data_path = config['data_paths']['processed_data']\n",
    "    season = config['analysis']['season']\n",
    "    \n",
    "    logger.info(f\"\\nAnalyzing season: {season}\")\n",
    "    \n",
    "    return config, raw_data_path, processed_data_path, season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_files(raw_data_path):\n",
    "    \"\"\"Define and check if expected data files exist in the raw data directory.\"\"\"\n",
    "    # Define expected data files\n",
    "    expected_files = {\n",
    "        'preseason_rankings': 'F00  2024 PreSeason Rankings.csv',\n",
    "        'season_data': 'F01  2023 2024 Season Player Data.csv',\n",
    "        'passing_data': 'F02  2023 2024 pff_passing_summary.csv',\n",
    "        'line_data': 'F03  2023 2024 pff_line_pass_blocking_efficiency.csv',\n",
    "        'receiving_data': 'F04  2023 2024 pff_receiving_summary.csv',\n",
    "        'rushing_data': 'F05  2023 2024 pff_rushing_summary.csv',\n",
    "        'team_stats': 'F06  2024 2023 Team Stats.csv'\n",
    "    }\n",
    "    \n",
    "    # Check if files exist\n",
    "    missing_files = []\n",
    "    for key, filename in expected_files.items():\n",
    "        file_path = os.path.join(raw_data_path, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_files.append(filename)\n",
    "            logger.warning(f\"⚠️ Missing file: {filename}\")\n",
    "        else:\n",
    "            logger.info(f\"✅ Found file: {filename}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        logger.warning(f\"\\n❌ {len(missing_files)} file(s) missing. Please add them to the {raw_data_path} directory.\")\n",
    "        logger.info(\"Continuing with available files...\")\n",
    "    else:\n",
    "        logger.info(f\"\\n✅ All expected files found in {raw_data_path}\")\n",
    "    \n",
    "    return expected_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_examine_data(config):\n",
    "    \"\"\"Load all data files and examine their structure.\"\"\"\n",
    "    # Load all data files\n",
    "    logger.info(\"Loading data files...\")\n",
    "    data_dict = load_all_data(config)\n",
    "    \n",
    "    # Print summary of loaded data\n",
    "    logger.info(\"\\nData summary:\")\n",
    "    for key, df in data_dict.items():\n",
    "        logger.info(f\"{key}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    # Validate dataframes\n",
    "    for key, df in data_dict.items():\n",
    "        validate_dataframe(df, key)\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_preseason_rankings(data_dict):\n",
    "    \"\"\"Examine the preseason rankings dataset.\"\"\"\n",
    "    if 'preseason_rankings' not in data_dict:\n",
    "        logger.warning(\"Preseason rankings data not available\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Examining preseason rankings data...\")\n",
    "    rankings_df = data_dict['preseason_rankings']\n",
    "    \n",
    "    logger.info(\"Preseason Rankings Sample:\")\n",
    "    print(rankings_df.head())\n",
    "    logger.info(f\"Columns: {rankings_df.columns.tolist()}\")\n",
    "    \n",
    "    # Show positions and teams\n",
    "    logger.info(f\"\\nPositions: {rankings_df['Position'].unique().tolist()}\")\n",
    "    logger.info(f\"Number of teams: {rankings_df['Team'].nunique()}\")\n",
    "    \n",
    "    # Show ADP distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=rankings_df, x='ADP', hue='Position', bins=20, multiple='stack')\n",
    "    plt.title('ADP Distribution by Position')\n",
    "    plt.savefig('outputs/figures/adp_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(\"ADP distribution histogram saved to outputs/figures/adp_distribution.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_season_data(data_dict, season):\n",
    "    \"\"\"Examine the season player data.\"\"\"\n",
    "    if 'season_data' not in data_dict:\n",
    "        logger.warning(\"Season data not available\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Examining season data...\")\n",
    "    season_data = data_dict['season_data']\n",
    "    \n",
    "    logger.info(\"Season Data Sample:\")\n",
    "    print(season_data.head())\n",
    "    \n",
    "    # Show seasons included\n",
    "    seasons = season_data['Season'].unique()\n",
    "    logger.info(f\"Seasons included: {seasons}\")\n",
    "    \n",
    "    # Count players by position\n",
    "    position_counts = season_data.groupby(['Season', 'FantPos']).size().unstack()\n",
    "    logger.info(\"\\nPlayers by position and season:\")\n",
    "    print(position_counts)\n",
    "    \n",
    "    # Show top performers by position for the target season\n",
    "    season_df = season_data[season_data['Season'] == season]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, pos in enumerate(['QB', 'RB', 'WR', 'TE']):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        pos_df = season_df[season_df['FantPos'] == pos].sort_values('TD', ascending=False).head(10)\n",
    "        sns.barplot(data=pos_df, x='Player', y='TD')\n",
    "        plt.title(f'Top 10 {pos}s by Touchdowns')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/figures/top_td_scorers.png')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(\"Top TD scorers chart saved to outputs/figures/top_td_scorers.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_passing_data(data_dict, season):\n",
    "    \"\"\"Examine the passing data.\"\"\"\n",
    "    if 'passing_data' not in data_dict:\n",
    "        logger.warning(\"Passing data not available\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Examining passing data...\")\n",
    "    passing_data = data_dict['passing_data']\n",
    "    \n",
    "    logger.info(\"Passing Data Sample:\")\n",
    "    print(passing_data.head())\n",
    "    \n",
    "    # Show seasons included\n",
    "    passing_seasons = passing_data['season'].unique()\n",
    "    logger.info(f\"Seasons included: {passing_seasons}\")\n",
    "    \n",
    "    # Show QBs by team\n",
    "    qb_by_team = passing_data[passing_data['season'] == season].groupby('team_name').size()\n",
    "    logger.info(f\"\\nNumber of QBs per team (season {season}):\")\n",
    "    print(qb_by_team.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_team_stats(data_dict, season):\n",
    "    \"\"\"Examine the team stats data.\"\"\"\n",
    "    if 'team_stats' not in data_dict:\n",
    "        logger.warning(\"Team stats not available\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Examining team stats...\")\n",
    "    team_stats = data_dict['team_stats']\n",
    "    \n",
    "    logger.info(\"Team Stats Sample:\")\n",
    "    print(team_stats.head())\n",
    "    \n",
    "    # Show seasons included\n",
    "    team_seasons = team_stats['Season'].unique()\n",
    "    logger.info(f\"Seasons included: {team_seasons}\")\n",
    "    \n",
    "    # Show teams with highest points scored\n",
    "    top_scoring_teams = team_stats[team_stats['Season'] == season].sort_values('PF', ascending=False).head(10)\n",
    "    logger.info(\"\\nTop 10 scoring teams:\")\n",
    "    print(top_scoring_teams[['Team (Full)', 'PF', 'Passing Yds', 'Rushing Yds']])\n",
    "    \n",
    "    # Visualize team offensive stats\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    season_teams = team_stats[team_stats['Season'] == season]\n",
    "    plt.scatter(season_teams['Passing Yds'], season_teams['Rushing Yds'], s=season_teams['PF']/2, alpha=0.7)\n",
    "    plt.xlabel('Passing Yards')\n",
    "    plt.ylabel('Rushing Yards')\n",
    "    plt.title(f'Team Offensive Profile (Size = Points Scored, Season {season})')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.savefig('outputs/figures/team_offensive_profile.png')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(\"Team offensive profile chart saved to outputs/figures/team_offensive_profile.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_team_names_across_datasets(data_dict):\n",
    "    \"\"\"Standardize team names across all datasets.\"\"\"\n",
    "    # Check team name variations\n",
    "    logger.info(\"Team name variations across datasets:\")\n",
    "    team_variations = {}\n",
    "    \n",
    "    for key, df in data_dict.items():\n",
    "        if 'Team' in df.columns:\n",
    "            teams = df['Team'].unique()\n",
    "            team_variations[key] = teams\n",
    "        elif 'team_name' in df.columns:\n",
    "            teams = df['team_name'].unique()\n",
    "            team_variations[key] = teams\n",
    "    \n",
    "    for dataset, teams in team_variations.items():\n",
    "        logger.info(f\"\\n{dataset}: {sorted(teams.tolist())}\")\n",
    "    \n",
    "    # Standardize team names\n",
    "    logger.info(\"\\nStandardizing team names...\")\n",
    "    data_dict = standardize_team_names(data_dict)\n",
    "    \n",
    "    # Verify standardization\n",
    "    logger.info(\"Verifying team name standardization:\")\n",
    "    for key, df in data_dict.items():\n",
    "        if 'Team_std' in df.columns:\n",
    "            std_teams = df['Team_std'].unique()\n",
    "            logger.info(f\"\\n{key}: {sorted(std_teams.tolist())}\")\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_target_season(data_dict, season):\n",
    "    \"\"\"Filter all datasets to include only the target season.\"\"\"\n",
    "    logger.info(f\"Filtering data for season {season}...\")\n",
    "    filtered_data_dict = filter_season_data(data_dict, season)\n",
    "    \n",
    "    # Print summary of filtered data\n",
    "    logger.info(\"\\nFiltered data summary:\")\n",
    "    for key, df in filtered_data_dict.items():\n",
    "        logger.info(f\"{key}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "        # Verify filtering worked\n",
    "        if 'Season' in df.columns and df['Season'].nunique() > 1:\n",
    "            logger.warning(f\"  ⚠️ WARNING: Multiple seasons still present in {key}: {df['Season'].unique()}\")\n",
    "        elif 'season' in df.columns and df['season'].nunique() > 1:\n",
    "            logger.warning(f\"  ⚠️ WARNING: Multiple seasons still present in {key}: {df['season'].unique()}\")\n",
    "    \n",
    "    return filtered_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_examine_master_dataset(filtered_data_dict):\n",
    "    \"\"\"Create a master player dataset and examine its properties.\"\"\"\n",
    "    logger.info(\"Creating master player dataset...\")\n",
    "    master_df = create_master_player_dataset(filtered_data_dict)\n",
    "    \n",
    "    # Check the result\n",
    "    logger.info(f\"Master dataset created with {master_df.shape[0]} rows and {master_df.shape[1]} columns\")\n",
    "    \n",
    "    # Display sample\n",
    "    logger.info(\"\\nMaster dataset sample:\")\n",
    "    print(master_df.head())\n",
    "    \n",
    "    # Show column names\n",
    "    logger.info(\"\\nColumns in master dataset:\")\n",
    "    column_groups = {}\n",
    "    for col in master_df.columns:\n",
    "        prefix = col.split('_')[0] if '_' in col else col\n",
    "        if prefix not in column_groups:\n",
    "            column_groups[prefix] = []\n",
    "        column_groups[prefix].append(col)\n",
    "    \n",
    "    for prefix, cols in column_groups.items():\n",
    "        logger.info(f\"\\n{prefix} columns:\")\n",
    "        for col in cols:\n",
    "            logger.info(f\"  - {col}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = master_df.isnull().sum()\n",
    "    missing_pct = (missing_values / len(master_df)) * 100\n",
    "    \n",
    "    logger.info(\"\\nColumns with >20% missing values:\")\n",
    "    high_missing = missing_pct[missing_pct > 20].sort_values(ascending=False)\n",
    "    for col, pct in high_missing.items():\n",
    "        logger.info(f\"  - {col}: {pct:.1f}% missing\")\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_examine_fantasy_points(master_df):\n",
    "    \"\"\"Calculate half-PPR fantasy points and examine the results.\"\"\"\n",
    "    logger.info(\"Calculating half-PPR fantasy points...\")\n",
    "    master_df = calculate_half_ppr_points(master_df)\n",
    "    \n",
    "    # Verify calculation\n",
    "    if 'Half_PPR' in master_df.columns:\n",
    "        logger.info(f\"Half-PPR points calculated for {master_df['Half_PPR'].notnull().sum()} players\")\n",
    "        \n",
    "        # Show top scorers by position\n",
    "        positions = ['QB', 'RB', 'WR', 'TE']\n",
    "        for pos in positions:\n",
    "            pos_df = master_df[master_df['FantPos'] == pos].sort_values('Half_PPR', ascending=False).head(10)\n",
    "            logger.info(f\"\\nTop 10 {pos}s by Half-PPR points:\")\n",
    "            print(pos_df[['Player', 'Team', 'Half_PPR', 'Half_PPR_PPG', 'G']])\n",
    "        \n",
    "        # Plot distribution of half-PPR points by position\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, pos in enumerate(positions):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            pos_df = master_df[master_df['FantPos'] == pos]\n",
    "            sns.histplot(pos_df['Half_PPR'], kde=True)\n",
    "            plt.title(f'Half-PPR Points Distribution - {pos}')\n",
    "            plt.xlabel('Half-PPR Points')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('outputs/figures/half_ppr_distribution.png')\n",
    "        plt.close()\n",
    "        \n",
    "        logger.info(\"Half-PPR point distributions saved to outputs/figures/half_ppr_distribution.png\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Half-PPR points calculation failed!\")\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_preseason_and_actual(master_df):\n",
    "    \"\"\"Compare pre-season rankings with actual performance.\"\"\"\n",
    "    if 'ADP' in master_df.columns and 'Half_PPR' in master_df.columns:\n",
    "        logger.info(\"Comparing pre-season rankings with actual performance...\")\n",
    "        \n",
    "        # Create scatter plot of ADP vs. actual performance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.scatterplot(data=master_df, x='ADP', y='Half_PPR', hue='FantPos', alpha=0.7)\n",
    "        plt.title('ADP vs. Actual Half-PPR Points')\n",
    "        plt.xlabel('Average Draft Position (ADP)')\n",
    "        plt.ylabel('Half-PPR Points')\n",
    "        \n",
    "        # Add trendline\n",
    "        from scipy.stats import linregress\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(master_df['ADP'].dropna(), master_df['Half_PPR'].dropna())\n",
    "        x = np.array([min(master_df['ADP'].dropna()), max(master_df['ADP'].dropna())])\n",
    "        plt.plot(x, intercept + slope * x, 'k--', alpha=0.7)\n",
    "        plt.text(0.05, 0.95, f'R² = {r_value**2:.2f}', transform=plt.gca().transAxes)\n",
    "        \n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.savefig('outputs/figures/adp_vs_performance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        logger.info(\"ADP vs. performance chart saved to outputs/figures/adp_vs_performance.png\")\n",
    "        \n",
    "        # Identify biggest overperformers and underperformers\n",
    "        master_df['ADP_Rank'] = master_df['ADP'].rank()\n",
    "        master_df['Performance_Rank'] = master_df['Half_PPR'].rank(ascending=False)\n",
    "        master_df['ADP_vs_Performance'] = master_df['ADP_Rank'] - master_df['Performance_Rank']\n",
    "        \n",
    "        # Show top overperformers\n",
    "        logger.info(\"Top 10 Overperformers (ADP vs Actual Performance):\")\n",
    "        overperformers = master_df.sort_values('ADP_vs_Performance', ascending=False).head(10)\n",
    "        print(overperformers[['Player', 'FantPos', 'Team', 'ADP', 'Half_PPR', 'ADP_Rank', 'Performance_Rank', 'ADP_vs_Performance']])\n",
    "        \n",
    "        # Show top underperformers\n",
    "        logger.info(\"\\nTop 10 Underperformers (ADP vs Actual Performance):\")\n",
    "        underperformers = master_df.sort_values('ADP_vs_Performance', ascending=True).head(10)\n",
    "        print(underperformers[['Player', 'FantPos', 'Team', 'ADP', 'Half_PPR', 'ADP_Rank', 'Performance_Rank', 'ADP_vs_Performance']])\n",
    "    else:\n",
    "        logger.warning(\"⚠️ Missing ADP or Half-PPR data for comparison!\")\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_datasets(master_df, filtered_data_dict, processed_data_path):\n",
    "    \"\"\"Save processed datasets for further analysis.\"\"\"\n",
    "    # Create outputs directory if it doesn't exist\n",
    "    os.makedirs('outputs/figures', exist_ok=True)\n",
    "    \n",
    "    # Save master player dataset\n",
    "    logger.info(f\"Saving processed data to {processed_data_path}...\")\n",
    "    save_processed_data(master_df, 'master_player_data.csv', processed_data_path)\n",
    "    \n",
    "    # Save individual datasets\n",
    "    # Save season data\n",
    "    if 'season_data' in filtered_data_dict:\n",
    "        save_processed_data(filtered_data_dict['season_data'], 'season_data.csv', processed_data_path)\n",
    "        logger.info(\"Saved season data\")\n",
    "    \n",
    "    # Save team stats\n",
    "    if 'team_stats' in filtered_data_dict:\n",
    "        save_processed_data(filtered_data_dict['team_stats'], 'team_stats.csv', processed_data_path)\n",
    "        logger.info(\"Saved team stats\")\n",
    "    \n",
    "    # Check if files were saved\n",
    "    expected_processed_files = [\n",
    "        'master_player_data.csv',\n",
    "        'season_data.csv',\n",
    "        'team_stats.csv'\n",
    "    ]\n",
    "    \n",
    "    logger.info(\"\\nVerifying saved files:\")\n",
    "    for filename in expected_processed_files:\n",
    "        file_path = os.path.join(processed_data_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            logger.info(f\"✅ {filename} saved successfully\")\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "            logger.info(f\"   File size: {file_size:.2f} MB\")\n",
    "        else:\n",
    "            logger.warning(f\"❌ Failed to save {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_and_next_steps(master_df, data_dict, season):\n",
    "    \"\"\"Print a summary of what was accomplished and outline next steps.\"\"\"\n",
    "    # Summary\n",
    "    logger.info(\"Data Preparation Summary:\\n\")\n",
    "    logger.info(f\"1. Loaded {len(data_dict)} datasets\")\n",
    "    logger.info(f\"2. Standardized team names across all datasets\")\n",
    "    logger.info(f\"3. Filtered data for the {season} season\")\n",
    "    logger.info(f\"4. Created master player dataset with {master_df.shape[0]} players and {master_df.shape[1]} columns\")\n",
    "    logger.info(f\"5. Calculated half-PPR fantasy points for {master_df['Half_PPR'].notnull().sum()} players\")\n",
    "    logger.info(f\"6. Saved processed data files\")\n",
    "    \n",
    "    logger.info(\"\\nNext Steps:\")\n",
    "    logger.info(\"1. Perform player performance analysis (01_player_performance.ipynb)\")\n",
    "    logger.info(\"2. Analyze positional value and scarcity (02_positional_value.ipynb)\")\n",
    "    logger.info(\"3. Evaluate draft value (03_draft_value.ipynb)\")\n",
    "    logger.info(\"4. Identify player tiers and archetypes (04_tiering_archetypes.ipynb)\")\n",
    "    logger.info(\"5. Analyze team context impact (05_team_context.ipynb)\")\n",
    "    logger.info(\"6. Evaluate advanced metrics (06_advanced_metrics.ipynb)\")\n",
    "    logger.info(\"7. Develop draft strategy framework (07_draft_strategy.ipynb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to execute the data preparation workflow.\"\"\"\n",
    "    # Set pandas display options\n",
    "    pd.set_option('display.max_columns', 50)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    \n",
    "    # 1. Load configuration\n",
    "    config, raw_data_path, processed_data_path, season = load_configuration()\n",
    "    \n",
    "    # 2. Check data files\n",
    "    expected_files = check_data_files(raw_data_path)\n",
    "    \n",
    "    # 3. Load data files\n",
    "    data_dict = load_and_examine_data(config)\n",
    "    \n",
    "    # 4. Examine key datasets\n",
    "    examine_preseason_rankings(data_dict)\n",
    "    examine_season_data(data_dict, season)\n",
    "    examine_passing_data(data_dict, season)\n",
    "    examine_team_stats(data_dict, season)\n",
    "    \n",
    "    # 5. Standardize team names\n",
    "    data_dict = standardize_team_names_across_datasets(data_dict)\n",
    "    \n",
    "    # 6. Filter for target season\n",
    "    filtered_data_dict = filter_for_target_season(data_dict, season)\n",
    "    \n",
    "    # 7. Create master dataset\n",
    "    master_df = create_and_examine_master_dataset(filtered_data_dict)\n",
    "    \n",
    "    # 8. Calculate fantasy points\n",
    "    master_df = calculate_and_examine_fantasy_points(master_df)\n",
    "    \n",
    "    # 9. Compare pre-season rankings with actual performance\n",
    "    master_df = compare_preseason_and_actual(master_df)\n",
    "    \n",
    "    # 10. Save processed datasets\n",
    "    save_processed_datasets(master_df, filtered_data_dict, processed_data_path)\n",
    "    \n",
    "    # 11. Print summary and next steps\n",
    "    print_summary_and_next_steps(master_df, data_dict, season)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
